
简单方式
val dataframe = rdd.map(r=>(r(0),r(1),r(2))).toDF("product_id","product_name","product_descrption")   //这种方式生成的schema比较简单
                            //(r(0),r(1),r(2)) 需要转化的字段
                            //（"product_id","product_name","product_descrption"）生成的新字段

复杂方式
步骤： 第一步：rdd转化成rowRDD & 生成schema   第二步： spark.createDataFrame(rowRDD,schema)
生成rowRDD:
方式一：rowRDD = rdd.map(_.split(",")).map(x => Row(x(0), x(1).trim))
方式二：rowRDD= rdd.map(x=>Row.ftamSeq(r))
        rowRDD=rdd.map(x=>Row(r(0).toInt,r(0).toDouble))   //注意有时候需要进行类型转化
生成schema：
val schema = StructType(Array(StructField(columName1,type,bool)))
实例 val schema =  StructType(Array(StructField("product_id",IntegerType,false),StructField("product_name",StringType,true)))
生成dataframe
val dataframe=createDataFrame(rowRDD,schema)



实例

scala> import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

scala> import org.apache.spark.sql.Encoder
import org.apache.spark.sql.Encoder

scala> import spark.implicits._  //导入包，支持把一个RDD隐式转换为一个DataFrame
import spark.implicits._

scala> case class Person(name: String, age: Long)  //定义一个case class
defined class Person

scala> val peopleDF = spark.sparkContext.textFile("file:///usr/local/spark/examples/src/main/resources/people.txt").map(_.split(",")).map(attributes => Person(attributes(0), attributes(1).trim.toInt)).toDF()
peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: bigint]

scala> peopleDF.createOrReplaceTempView("people")  //必须注册为临时表才能供下面的查询使用

scala> val personsRDD = spark.sql("select name,age from people where age > 20")
//最终生成一个DataFrame
personsRDD: org.apache.spark.sql.DataFrame = [name: string, age: bigint]
scala> personsRDD.map(t => "Name:"+t(0)+","+"Age:"+t(1)).show()  //DataFrame中的每个元素都是一行记录，包含name和age两个字段，分别用t(0)和t(1)来获取值





